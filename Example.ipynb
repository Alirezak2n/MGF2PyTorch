{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "import MGF2PyTorch\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [1] Parsing MGF files\n",
    "I implemented a class called `MGF2PyTorch` that supports multiple methods for parsing, storing, and loading ms/ms spectra from MGF files and convert them into a format suitable for PyTorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "converter = MGF2PyTorch.MGF2PyTorch()\n",
    "mgf_file = \"b1938_293T_proteinID_02B_QE3_122212.mgf\"\n",
    "hdf5_file = \"output_flat.hdf5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first parser is a manual parser that reads the MGF file line by line and extracts the relevant information. This parser is fast, making it ideal for large files. It is also flexible, allowing for easy modifications to the parsing logic if needed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual parsing took 12.76 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "spectra_manual = converter.parse_mgf(mgf_file)\n",
    "end_time = time.time()\n",
    "print(f\"Manual parsing took {end_time - start_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second parser uses the `pyteomics` library to read the MGF file. This appriach provides a standardized way to read MGF files and is useful for compatibility with other tools."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyteomics parsing took 14.99 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "spectra_pyteomics = converter.read_mgf_pyteomics(mgf_file)\n",
    "end_time = time.time()\n",
    "print(f\"Pyteomics parsing took {end_time - start_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The third parsing function combines manual parser with storing to an HDF5 file using a flat representation. The following fields are flattened and stored: m/z values, Intensities, Peak list lengths, Charges, precursor m/z values, and retention times."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing to HDF5 took 12.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "converter.parse_and_store_flat_hdf5(mgf_file, hdf5_file)\n",
    "end_time = time.time()\n",
    "print(f\"Storing to HDF5 took {end_time - start_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [2] Querying the spectra\n",
    "In the MGF2PyTorch class, I implemented two example functions of `filter_spectra_by_charge` and `filter_peaks_by_mz` to demonstrate meta-data filtering and peak filtering. These functions are useful for quickly extracting relevant information before model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Peaks:\n",
      " [[  112.08773   4129.667   ]\n",
      " [  113.07166   1194.3569  ]\n",
      " [  115.08704  52450.684   ]\n",
      " [  115.094635  2902.2378  ]\n",
      " [  116.01717   2280.9895  ]\n",
      " [  116.091225  1693.554   ]\n",
      " [  129.10263  49685.86    ]\n",
      " [  129.11165   3384.201   ]\n",
      " [  130.0868   10056.626   ]\n",
      " [  133.04329  15631.203   ]\n",
      " [  137.07132   1505.9761  ]\n",
      " [  139.08687   1986.8506  ]\n",
      " [  147.1128   21817.438   ]\n",
      " [  148.34206   1413.1466  ]\n",
      " [  149.5296    1493.5461  ]\n",
      " [  158.09259   1871.8987  ]\n",
      " [  161.03833   1878.8705  ]\n",
      " [  165.066     2564.3567  ]\n",
      " [  167.08179   3148.0422  ]\n",
      " [  171.05885   4739.223   ]\n",
      " [  175.1193   14015.725   ]\n",
      " [  178.0644    4805.7373  ]\n",
      " [  185.07455   3086.0195  ]\n",
      " [  186.12479   1802.6827  ]\n",
      " [  188.03183   1586.6492  ]]\n",
      "Spectrum title:\n",
      " b1938_293T_proteinID_02B_QE3_122212.1701.1701.2\n"
     ]
    }
   ],
   "source": [
    "# Filter spectra to select only those with a charge of 2.\n",
    "high_charge_spectra = MGF2PyTorch.filter_spectra_by_charge(spectra_manual, charge_value=2)\n",
    "filtered_spectra = MGF2PyTorch.filter_peaks_by_mz(spectra_manual[0], 100, 200)\n",
    "print(\"Filtered Peaks:\\n\", filtered_spectra)\n",
    "print(\"Spectrum title:\\n\", spectra_manual[0]['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [3] PyTorch integration\n",
    "I implemented a custom SpectraDataset class to wrap the HDF5 data for use in PyTorch pipelines. This class includes context management methods (__enter__ and __exit__) to ensure that HDF5 file handles are properly managed and closed, preventing resource leaks during extended training sessions. Furthermore, a custom collate function is used in the DataLoader to pad variable-length sequences, thereby ensuring uniform batch processing for training neural network models. This makes the dataset compatible with PyTorch's DataLoader for efficient and scalable data loading."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectra in HDF5: 45642\n",
      "Batch keys: dict_keys(['mz', 'intensity', 'charge', 'precursor', 'rt'])\n",
      "Batch m/z shape: torch.Size([4, 422])\n",
      "Batch intensity shape: torch.Size([4, 422])\n",
      "Batch charge: tensor([2, 2, 2, 4])\n",
      "Batch precursor: tensor([360.6677, 317.1633, 322.1191, 356.4366])\n",
      "Batch retention time: tensor([1499.2566, 1501.8916, 1503.2412, 1668.9448])\n"
     ]
    }
   ],
   "source": [
    "with MGF2PyTorch.SpectraDataset(hdf5_file) as dataset:\n",
    "    print(f\"Number of spectra in HDF5: {len(dataset)}\")\n",
    "    dataloader = DataLoader(dataset, batch_size=4, collate_fn=MGF2PyTorch.collate_fn, shuffle=False)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch keys:\", batch.keys())\n",
    "        print(\"Batch m/z shape:\", batch['mz'].shape)\n",
    "        print(\"Batch intensity shape:\", batch['intensity'].shape)\n",
    "        print(\"Batch charge:\", batch['charge'])\n",
    "        print(\"Batch precursor:\", batch['precursor'])\n",
    "        print(\"Batch retention time:\", batch['rt'])\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance\n",
    "## Parsing efficiency\n",
    "The manual parser is approximately 15% faster than the pyteomics parser which is the reason I implemented it within the third parser which combines it with flatting and directly stores data to HDF5 file. This avoids redundant parsing steps and keeps memory usage low.\n",
    "## I/O efficiency\n",
    "The HDF5 file format is optimized for fast I/O operations, making it suitable for large datasets. I chose this method as it is efficient especially in machine learning applications. Another method was combination of Numpy arrays and Parquet but based on the use case I chose this approach as it better supports variable-length spectra in a single file.\n",
    "## Scalability and resource management\n",
    "The integration of context management within the SpectraDataset class ensures that resources (e.g., file handles) are correctly managed even in long-running sessions or when processing multiple files in parallel. I chose it to minimal I/O bottlenecks and to ensure that the system can handle larger datasets without running into memory issues. This is particularly important in machine learning applications where large datasets are common."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
